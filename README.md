# AI Chat Flask Ollama Redis

A lightweight **local AI chat API** built with **Flask**, **Ollama**, and **Redis** â€” enabling you to run and cache **LLM (Large Language Model)** responses locally without relying on external APIs like OpenAI.

---

## ğŸš€ Features
- âš¡ **Flask-based REST API** for chatting with an LLM
- ğŸ§© **Ollama integration** for running models locally (e.g. Llama 3, Mistral, Gemma)
- ğŸ§  **Redis caching** to store and reuse recent responses
- ğŸ” **Stateless or contextual chat** options
- ğŸª¶ Lightweight and easy to deploy anywhere

---

## ğŸ› ï¸ Tech Stack
- **Flask** â€“ web server and routing
- **Ollama** â€“ local model inference engine
- **Redis** â€“ caching for fast repeated queries
- **Python 3.10+**

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/jawwadabbasi/ai-chat-flask-ollama-redis.git
cd ai-chat-flask-ollama-redis
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Start Redis
If Redis is installed via Homebrew:
```bash
brew services start redis
```

### 5ï¸âƒ£ Run Ollama
Make sure you have [Ollama](https://ollama.com/) installed and a model pulled:
```bash
ollama pull llama3
```

### 6ï¸âƒ£ Run the Flask App
```bash
python3 app.py
```

---

## ğŸ§© API Endpoints

### `POST /api/v1/chat`
Send a message to the local model and receive a generated response.

**Request:**
```json
{
  "prompt": "Explain einstein's theory of relativity in simple terms"
}
```

**Response:**
```json
{
  "cached": false,
  "response": "Albert Einstein developed the Theory of Relativity which is actually two theories: Special and General. Let me explain them simply for you.\n\nSpecial Relativity, published by Einstein in 1905, basically tells us about space and time when we are not dealing with things that have a lot of gravity (like stars or planets). Imagine if everyone saw the world differently based on how fast they were moving â€“ this is what Special Relativity says. There's also an interesting thing called \"time dilation,\" which means two clocks can run at different speeds relative to each other when you move them really fast (close to light speed).\n\nGeneral Relativity, published later in 1915, extends these ideas and tells us about space and time near very massive objects. Think of gravity as a sort of \"dent\" or warp in the fabric of our universe caused by big things like stars and planets â€“ this is General Relativity's idea using what we can now understand to be spacetime being curved around mass.\n\nA famous result from these theories was that Einstein predicted light could bend because it moves through space which gets bent, something called gravitational lensing which has been observed with stars and galaxies far away in the universe when their light passes close by other large objects on its way to Earth."
}
```

---

## ğŸ’¾ Redis Caching
Responses are automatically cached using the prompt as the key:
- The next time you send the same prompt, itâ€™ll return instantly from Redis.
- To clear cache, run:
```bash
redis-cli FLUSHALL
```

---

## ğŸ§  Example Curl Command
```bash
curl -X POST http://localhost:8000/api/v1/chat \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Tell me something about the moon"}'
```

---

## ğŸŒ Example Use Cases
- Build your **own ChatGPT-like API** without internet dependency
- Integrate into a **local research assistant**
- Cache responses for **offline chatbots or CLI tools**
- Extend with **RAG (Retrieval-Augmented Generation)** later

---

## ğŸ“ Project Structure
```
ai-chat-flask-ollama-redis/
â”‚
â”œâ”€â”€ app.py                 # Main Flask app
â”œâ”€â”€ includes/
â”‚   â”œâ”€â”€ redis.py           # Redis cache handler
â”‚   â””â”€â”€ ollama_client.py   # Ollama integration logic
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ License
This project is licensed under the **MIT License** â€” free for personal and commercial use.
"""